---
title: "The Application of Machine Learning on Risk Classification of Heart Disease"
author:
- Xiaoyu Lin
- Kyoka Ono
output:
  pdf_document: default
  html_document:
    df_print: paged
geometry: margin=0.75in
fontsize: 11pt
---
GitHub Link: <https://github.com/kyokaono8811/biostat625finalproject.git>

Contributions: 

-   Xiaoyu Lin — "Data description, Data cleaning, Write report"\
-   Kyoka Ono — "Model Training, Evaluation Metrics, Write report"

# Abstract

Heart disease is a leading cause of death for adults in the U.S, and early detection of key risk factors is essential for prevention. In this study, we apply multiple machine learning methods, including logistic regression, random forest, and GAM to identify the significant risk factors of heart disease. We then evaluate model performance and compare variable importance across methods using evaluation Metrics.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment = "",
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

```{r}
#install.packages("reticulate")
library(reticulate)
```

```{r}
#reticulate::py_install("matplotlib")
#reticulate::py_install("pandas")
#reticulate::py_install("seaborn")
```


```{python}
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('future.no_silent_downcasting', True)
```

# Introduction

Heart disease affects millions of individuals in the U.S, making early identification of risk factors a high public-health priority. Traditional statistical approaches have identified several predictors, but machine learning methods can capture more complex, nonlinear relationships.

This project applies several different machine learning algorithms to the `heart_2022_no_nans.csv` dataset from the CDC Behavioral Risk Factor Surveillance System (BRFSS) on Kaggle. Our goal is to determine which model outputs the highest evaluation metric for predicting heart attack.

# Methods

### **Data Source**

The dataset contains 40 variables for over 200,000 survey participants (all complete cases).

```{python}
heart_data = pd.read_csv("data/heart_2022_no_nans.csv")

# Number of participants (rows) and variables (columns)
print("\nNumber of participants:", heart_data.shape[0])
print("Number of variables:", heart_data.shape[1])
```

### **Preprocessing**

**Variable Selection**

We choose 10 significant covariates out of 40 for our models based on literature review and use `HadHeartAttack` as the predicted variable

```{python}
heart_data = heart_data[[
        "HadHeartAttack",
        "Sex",
        "PhysicalActivities",
        "SleepHours",
        "HadStroke",
        "HadDiabetes",
        "SmokerStatus",
        "RaceEthnicityCategory",
        "AgeCategory",
        "BMI",
        "AlcoholDrinkers"]
]
```

Outcome Variable:

-   `HadHeartAttack`: Binary indicator (Yes/No) of whether a doctor diagnosed the respondent with a heart attack.

Predictor Variables:

-   `Sex`: Biological sex of the participant (Male/Female).

-   `PhysicalActivities`: Whether the participant engaged in physical activities in the past month (Yes/No).

-   `SleepHours`: Average number of hours of sleep per night (numeric).

-   `HadStroke`: Whether had a stroke (Yes/No).

-   `HadDiabetes`: Whether had a diabetes (Yes/No/Yes, but only during pregnancy (female)/No, pre-diabetes or borderline diabetes).

-   `SmokerStatus`: Smoking status of the participant (Former smoker/Never smoked/Current smoker – now smokes every day/Current smoker – now smokes some days/No).

-   `RaceEthnicityCategory`: (White only, Non-Hispanic/Black only, Non-Hispanic/Other race only, Non-Hispanic/Multiracial, Non-Hispanic/Hispanic)

-   `AgeCategory`: Age group (18–24, 25–29, …, 80+).

-   `BMI`: Body mass index (numeric).

-   `AlcoholDrinkers`: Whether a participant is a heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per (Yes/No).

**Data Cleaning**

We would like to create dummy variables for categorical variables and merge equivalent levels. Specifically, we recode the `HadDiabetes` variable so that "Yes, but only during pregnancy (female)" is treated as "Yes", and "No, pre-diabetes or borderline diabetes" is treated as "No". And "Current smoker – now smokes every day"/"Current smoker – now smokes some days" both as "Current smoker".

```{python}
from analysis.cleaning import clean_data
heart_clean = clean_data(heart_data)
```

### **Data Description**

Let's look at the distribution of the variables

```{python}
# Variables
categorical_vars = [
    "HadHeartAttack", "Sex", "PhysicalActivities", "HadStroke",
    "HadDiabetes", "SmokerStatus", "RaceEthnicityCategory",
    "AgeCategory", "AlcoholDrinkers"
]
    
numeric_vars = ["SleepHours", "BMI"]
    
all_vars = categorical_vars + numeric_vars
    
# Order of categorical variables
category_orders = {
    "HadHeartAttack": [0, 1],
    "Sex": [0, 1],
    "PhysicalActivities": [0, 1],
    "HadStroke": [0, 1],
    "HadDiabetes": [0, 1],
    "AlcoholDrinkers": [0, 1],
    "SmokerStatus": [0, 1, 2],
    "RaceEthnicityCategory": [0, 1, 2, 3, 4],
    "AgeCategory": list(range(1, 14))
}
    
# Layout
n_plots = len(all_vars)
n_cols = 3
n_rows = (n_plots + n_cols - 1) // n_cols
    
fig, axes = plt.subplots(
    n_rows,
    n_cols,
    figsize=(20, 2.5 * n_rows),
    sharey=False
)
    
axes = axes.flatten()
    
for i, col in enumerate(all_vars):
    ax = axes[i]
    
    # Categorical variables: bar plots
    if col in categorical_vars:
        counts = heart_clean[col].value_counts().reindex(category_orders[col])
        ax.bar(counts.index, counts.values)
        ax.set_xticks(category_orders[col])
        ax.tick_params(axis="x", labelrotation=45, labelsize=10)
    
    # Numerical variables: histograms
    else:
        ax.hist(heart_clean[col], bins=20)
        ax.tick_params(axis="x", labelrotation=45, labelsize=10)
    
    ax.set_title(col, fontsize=12)
    ax.tick_params(axis="y", labelsize=10)
    ax.set_xlabel("")
    ax.set_ylabel("")
    
# Remove empty panels
for j in range(len(all_vars), len(axes)):
    fig.delaxes(axes[j])
    
# Shared y-axis label
fig.text(0.04, 0.5, "Count", va="center", rotation="vertical", fontsize=12)
    
# Adjust spacing
plt.subplots_adjust(
    wspace=0.2,  # horizontal space
    hspace=0.3   # vertical space
)
# Save the figure
fig.savefig("graphs/all_variables_plot.png", dpi=300, bbox_inches="tight")
plt.show()
```

From the graph above, we can see that some categorical variables such as `HadHeartAttack`, `HadStroke`, and `RaceEthnicityCategory` exhibit noticeable class imbalance. Other categorical variables are more balanced. Numerical variables like `SleepHours`, and `BMI` show skewed distributions. 

Now Let us build a correlation table. We use spearman correlation because the dataset contains a mixture of categorical and numerical variables. Spearman does not assume linearity or normality, making it a better measure of association than Pearson for this dataset.

```{python}
vars_of_interest = [
    "HadHeartAttack",
    "Sex",
    "PhysicalActivities",
    "SleepHours",
    "HadStroke",
    "HadDiabetes",
    "SmokerStatus",
    "RaceEthnicityCategory",
    "AgeCategory",
    "BMI",
    "AlcoholDrinkers"
]
    
df_corr = heart_clean[vars_of_interest].copy()
    
# Compute Spearman correlation
corr = df_corr.corr(method="spearman")
    
# Mask for upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))
    
fig, ax = plt.subplots(figsize=(7, 4))

sns.heatmap(
    corr,
    mask=mask,
    annot=True,
    fmt=".2f",
    cmap="coolwarm",
    vmin=-1, vmax=1,
    square=True,
    linewidths=0.5,
    annot_kws={"size": 6},
    cbar_kws={"shrink": 0.7},
    ax=ax
)

ax.set_xticklabels(ax.get_xticklabels(), fontsize=6, rotation=45, ha="right")
ax.set_yticklabels(ax.get_yticklabels(), fontsize=6)
ax.set_title("Correlation Table", fontsize=9)

cbar = ax.collections[0].colorbar
cbar.ax.tick_params(labelsize=6)

plt.tight_layout()

fig.savefig("graphs/correlation_table.png", dpi=300, bbox_inches="tight")
plt.show()
```

All pairwise correlations were relatively weak, indicating that no individual predictor shows a strong linear association with heart attack or with each other. This is expected in multi-factor health datasets, where the outcome is influenced by many small effects rather than a single dominant variable.

Low pairwise correlations do not imply weak predictive power for nonlinear effects, interactions, and combined contributions can still provide meaningful classification performance in multivariate models.

### **Models Applied**

Highly imbalanced classes may affect model performance if not addressed, so we will use the method of undersampling or class weights during model training. And to improve model performance, the skewed numerical variables should be standardized or centered, especially for algorithms sensitive to scale (e.g. logistic regression).

We will use logistic regression, random forest, and Logistic General Additive Model to build our models, then check wich method gives the most accurate model.

```{r}
#reticulate::py_install("scikit-learn")
#reticulate::py_install("imblearn")
#reticulate::py_install("tensorflow")
#reticulate::py_install("pygam")
#reticulate::py_install("shap")
```

```{python}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler as ss
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score
from imblearn.under_sampling import RandomUnderSampler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
```

```{python}
from pygam import LogisticGAM, s
import time 
```

```{python}
X = heart_clean.drop(columns=["HadHeartAttack"])
y = heart_clean["HadHeartAttack"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=123, stratify=y
)

# Undersampling
rus = RandomUnderSampler(random_state=42)
X_train_under, y_train_under = rus.fit_resample(X_train, y_train)

# Standardize
scaler = ss()

# Fit only on training data (after and before undersampling)
X_train_under[numeric_vars] = scaler.fit_transform(X_train_under[numeric_vars])
X_train[numeric_vars] = scaler.fit_transform(X_train[numeric_vars])

# Apply the same transformation to test data
X_test[numeric_vars] = scaler.transform(X_test[numeric_vars])
```

```{python, results = "hide"}
# Logistic Regression
# LogisticRegressionCV automatically 
start = time.time()
logistic = LogisticRegressionCV(max_iter=1000, cv=5, random_state=123).fit(X_train_under, y_train_under)
end = time.time()
print(f"Elapsed: {end - start:.3f} sec")
```

```{python}
# Predicting Values/Probabilities 
pred_logistic_test = logistic.predict(X_test) # only 0 or 1 
pred_logistic_proba_test = logistic.predict_proba(X_test) # actual probabilities (between 0 and 1)
```

```{python, results = "hide"}
# Random Forest
random_forest = RandomForestClassifier(random_state = 123)

start = time.time()
random_forest.fit(X_train_under, y_train_under)
end = time.time()

print(f"Elapsed: {end - start:.3f} seconds")
```

```{python, results = "hide"}
random_forest.get_params()
```

```{python}
# Predicting Values/Probabilities 
pred_rf_test = random_forest.predict(X_test) # only 0 or 1 
pred_rf_proba_test = random_forest.predict_proba(X_test) # actual probabilities (between 0 and 1)
```

```{python, results = "hide"}
# Logistic General Additive Model
gam = LogisticGAM(n_splines = 20) 

start = time.time()

gam.fit(X_train_under, y_train_under)

end = time.time()
print(f"Elapsed: {end - start:.3f} seconds")
```

```{python}
# Predicting Values/Probabilities 
pred_gam_test = gam.predict(X_test) # only 0 or 1 
pred_gam_proba_test = gam.predict_proba(X_test) # actual probabilities (between 0 and 1)
```

### **Evaluation Metrics**

```{python, results = "hide"}
# 1. Logistic Regression
# False Positive Rate, True Positive Rate 
fpr_test_logistic, tpr_test_logistic, thresholds_test_logistic = roc_curve(y_test, pred_logistic_proba_test[:,1])
        
# AUC
roc_auc_test_logistic = auc(fpr_test_logistic, tpr_test_logistic)
```

```{python, results = "hide"}
# 2. Random Forest
# Compute ROC curve and area under the curve
# False Positive Rate, True Positive Rate 
fpr_test_rf, tpr_test_rf, thresholds_test_rf = roc_curve(y_test, pred_rf_proba_test[:,1])
        
# AUC
roc_auc_test_rf = auc(fpr_test_rf, tpr_test_rf)
```

```{python, results = "hide"}
# 3. GAM
# Compute ROC curve and area under the curve
# False Positive Rate, True Positive Rate 
fpr_test_gam, tpr_test_gam, thresholds_test_gam = roc_curve(y_test, pred_gam_proba_test)
        
# AUC
roc_auc_test_gam = auc(fpr_test_gam, tpr_test_gam)
```

```{python, results = "hide"}
# Create ROC compare figure
plt.figure(figsize=(5, 3))

# Logistic Regression
plt.plot(fpr_test_logistic, tpr_test_logistic, color='limegreen', lw=2,
         label=f'Logistic Regression (AUC = {roc_auc_test_logistic:.2f})')

# Random Forest
plt.plot(fpr_test_rf, tpr_test_rf, color='royalblue', lw=2,
         label=f'Random Forest (AUC = {roc_auc_test_rf:.2f})')

# Logistic GAM
plt.plot(fpr_test_gam, tpr_test_gam, color='orange', lw=2,
         label=f'Logistic GAM (AUC = {roc_auc_test_gam:.2f})')

# Diagonal line (random classifier)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)

# Axes and labels
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate = 1 - Specificity', fontsize=8)
plt.ylabel('True Positive Rate = Sensitivity', fontsize=8)
plt.title('ROC Curves Comparison', fontsize=9)
plt.xticks(fontsize=7)
plt.yticks(fontsize=7)

# Legend and grid
plt.legend(loc='lower right', fontsize=7)
plt.grid(True)

# Save combined figure
plt.savefig("graphs/roc_combined.png", dpi=300, bbox_inches="tight")
plt.show()

```


# Results

```{python}
results = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest", "Logistic GAM"],
    "Accuracy": [
        accuracy_score(y_test, pred_logistic_test),
        accuracy_score(y_test, pred_rf_test),
        accuracy_score(y_test, pred_gam_test)
    ],
    "Precision": [
        precision_score(y_test, pred_logistic_test),
        precision_score(y_test, pred_rf_test),
        precision_score(y_test, pred_gam_test)
    ],
    "Recall": [
        recall_score(y_test, pred_logistic_test),
        recall_score(y_test, pred_rf_test),
        recall_score(y_test, pred_gam_test)
    ],
    "F1": [
        f1_score(y_test, pred_logistic_test),
        f1_score(y_test, pred_rf_test),
        f1_score(y_test, pred_gam_test)
    ],
    "AUROC": [
        roc_auc_score(y_test, pred_logistic_proba_test[:,1]),
        roc_auc_score(y_test, pred_rf_proba_test[:,1]),
        roc_auc_score(y_test, pred_gam_proba_test)
    ]
})

results

```

The logistic regression has an accuracy of 71%, precision of 13 %, recall of 76%, and F1 score of 22%. The AUROC is 81%. It took 0.5 seconds to run the model.

The random forest has an accuracy of 67%, precision of 11%, recall of  72%, and F1 score of 19%. The AUROC is 76%. The model training time was 1 second.

The Logistic Generative Additive Model has an accuracy of 71%, precision of 13%, recall of 77%, and F1 score of 22%. The AUROC is 81%. The time it took to train the model was 1 second.

# Conclusion

Logistic Regression has the highest accuracy metric, indicating that it has the highest correction rate for evaluating patients who both did and did not have a heart attack history. Random Forest shows the highest precision score, meaning it has the highest correction rate for evaluating patients who had a heart attack in real life out of the patients who were predicted to have heart attack. However, all of these models have significantly low precision scores, making them unreliable for precision evaluation. This result is anticipated because the heart attack has already happened in the past, and the variables are collected to represent association with heart attack history and not intended for prediction purposes. Logistic Regression has the highest F1 score, and it implies that it may be the optimal model for evaluating heart attack prevalence among the three models because F1 score takes the balance between precision and recall into account. All of the F1 scores are low, due to the significant low values in precision. But our models are good at finding people who do have heart disease, that is, the models can still meaningfully separate high-risk from low-risk patients..

# Future Work

Currently, most of the models use the default hyperparameters from the scikit-learn package; therefore, we can conduct hyperparameter tuning for model enhancement using methods such Grid Search and Random Search. In order to retrieve results with higher F1 scores, the state of the art (SOTA) models such as RNN and XgBoost may be deployed. However, these models are computationally expensive, and hyperparameter tuning for these models is time consuming and requires parallel computing tools like GPUs. Additionally, there is imbalance in predictor variables such as a history of diabetes patients, their smoking status, and race/ethnicity category. Therefore, this may lead the models to cause representation bias where minority groups are underrepresented while the model is going through the training phase. 

# References

-   Pytlak, K. (n.d.). Personal key indicators of heart disease [Data set]. Kaggle. <https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease/data>

-   Centers for Disease Control and Prevention. (2024, December 2). Heart disease risk factors. <https://www.cdc.gov/heart-disease/risk-factors/?CDC_AAref_Val=https://www.cdc.gov/heartdisease/risk_factors.htm>

-   Al-Zaiti, S.S., Martin-Gill, C., Zègre-Hemsey, J.K. et al. Machine learning for ECG diagnosis and risk stratification of occlusion myocardial infarction. Nat Med 29, 1804–1813 (2023). <https://doi.org/10.1038/s41591-023-02396-3>
